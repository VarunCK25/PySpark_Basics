{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating List of Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "donuts = [(\"plain donut\", 1.50, \"2018-04-17\"), (\"vanilla donut\", 2.0, \"2018-04-01\"), (\"glazed donut\", 2.50, \"2018-04-02\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(donuts).toDF(\"Donut Name\", \"Price\", \"Purchase Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+\n",
      "|   Donut Name|Price|Purchase Date|\n",
      "+-------------+-----+-------------+\n",
      "|  plain donut|  1.5|   2018-04-17|\n",
      "|vanilla donut|  2.0|   2018-04-01|\n",
      "| glazed donut|  2.5|   2018-04-02|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing String Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr, length, trim, ltrim, rtrim, reverse, substring, isnull, concat_ws, initcap, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying String Functions on Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+--------------+------+-------------+-------------+-------------+-------------+---------+------+-------------------+-------------+\n",
      "|   Donut Name|Price|Purchase Date|Contains plain|Length|         Trim|        LTrim|        RTrim|      Reverse|Substring|IsNull|             Concat|      InitCap|\n",
      "+-------------+-----+-------------+--------------+------+-------------+-------------+-------------+-------------+---------+------+-------------------+-------------+\n",
      "|  plain donut|  1.5|   2018-04-17|             7|    11|  plain donut|  plain donut|  plain donut|  tunod nialp|    plain| false|  plain donut - 1.5|  Plain Donut|\n",
      "|vanilla donut|  2.0|   2018-04-01|             9|    13|vanilla donut|vanilla donut|vanilla donut|tunod allinav|    vanil| false|vanilla donut - 2.0|Vanilla Donut|\n",
      "| glazed donut|  2.5|   2018-04-02|             8|    12| glazed donut| glazed donut| glazed donut| tunod dezalg|    glaze| false| glazed donut - 2.5| Glazed Donut|\n",
      "+-------------+-----+-------------+--------------+------+-------------+-------------+-------------+-------------+---------+------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.withColumn(\"Contains plain\", instr(\"Donut Name\", \"donut\"))\n",
    "  .withColumn(\"Length\", length(\"Donut Name\"))\n",
    "  .withColumn(\"Trim\", trim(col(\"Donut Name\")))\n",
    "  .withColumn(\"LTrim\", ltrim(col(\"Donut Name\")))\n",
    "  .withColumn(\"RTrim\", rtrim(col(\"Donut Name\")))\n",
    "  .withColumn(\"Reverse\", reverse(col(\"Donut Name\")))\n",
    "  .withColumn(\"Substring\", substring(\"Donut Name\", 0, 5))\n",
    "  .withColumn(\"IsNull\", isnull(\"Donut Name\"))\n",
    "  .withColumn(\"Concat\", concat_ws(\" - \", \"Donut Name\", \"Price\"))\n",
    "  .withColumn(\"InitCap\", initcap(\"Donut Name\"))\n",
    "  .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
